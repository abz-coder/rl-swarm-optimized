import logging  
import os
import sys
import time
from collections import defaultdict
from pathlib import Path

from genrl.blockchain import SwarmCoordinator
from genrl.communication import Communication
from genrl.communication.hivemind.hivemind_backend import HivemindBackend
from genrl.data import DataManager
from genrl.game import BaseGameManager
from genrl.game.game_manager import DefaultGameManagerMixin
from genrl.logging_utils.global_defs import get_logger
from genrl.logging_utils.system_utils import get_system_info
from genrl.rewards import RewardManager
from genrl.roles import RoleManager
from genrl.state import GameState
from genrl.trainer import TrainerModule
from huggingface_hub import login, whoami

from rgym_exp.src.utils.name_utils import get_name_from_peer_id


class SwarmGameManager(BaseGameManager, DefaultGameManagerMixin):
    """GameManager that orchestrates a game using a SwarmCoordinator."""

    def __init__(
        self,
        coordinator: SwarmCoordinator,
        max_stage: int,
        max_round: int,
        game_state: GameState,
        reward_manager: RewardManager,
        trainer: TrainerModule,
        data_manager: DataManager,
        communication: Communication,
        role_manager: RoleManager | None = None,
        run_mode: str = "train",
        log_dir: str = "logs",
        hf_token: str | None = None,
        hf_push_frequency: int = 20,
        **kwargs,
    ):

        super().__init__(
            max_stage=max_stage,
            max_round=max_round,
            game_state=game_state,
            reward_manager=reward_manager,
            trainer=trainer,
            data_manager=data_manager,
            communication=communication,
            role_manager=role_manager,
            run_mode=run_mode,
        )

        assert isinstance(self.communication, HivemindBackend)
        self.train_timeout = 60 * 60 * 24 * 31  # 1 month

        # –ü–æ–ª—É—á–∞–µ–º peer ID –æ—Ç HivemindBackend
        self.peer_id = self.communication.get_id()
        self.state.peer_id = self.peer_id
        self.animal_name = get_name_from_peer_id(self.peer_id, True)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ peer ID
        get_logger().info(f"üÜî Peer ID generated: {self.peer_id}")
        get_logger().info(f"üêæ Animal name: {self.animal_name}")

        # Logging Setup
        format_msg = f"[{self.animal_name}] %(asctime)s %(levelname)s: %(message)s"
        logging.basicConfig(level=logging.INFO, format=format_msg)
        formatter = logging.Formatter(format_msg)
        file_handler = logging.FileHandler(
            os.path.join(log_dir, f"training_{self.animal_name}.log")
        )
        file_handler.setFormatter(formatter)
        _LOG = get_logger()
        _LOG.addHandler(file_handler)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º coordinator –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–ª–æ–∫—á–µ–π–Ω–æ–º
        self.coordinator = coordinator
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–ª–æ–∫—á–µ–π–Ω –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ
        get_logger().info("üîó Initializing blockchain coordinator connection...")
        get_logger().info(f"üì° Blockchain coordinator type: {type(coordinator).__name__}")
        
        # –ü–æ–ª—É—á–∞–µ–º bootnodes –∏–∑ –±–ª–æ–∫—á–µ–π–Ω–∞
        try:
            get_logger().info("üì° Getting bootnodes from blockchain...")
            bootnodes = self.coordinator.get_bootnodes()
            get_logger().info(f"‚úÖ Successfully retrieved {len(bootnodes)} bootnodes from blockchain:")
            for i, bootnode in enumerate(bootnodes):
                get_logger().info(f"  üìç Bootnode {i+1}: {bootnode}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ bootnodes –Ω–µ –ø—É—Å—Ç—ã–µ
            if bootnodes:
                get_logger().info("‚úÖ Blockchain bootnodes available - system will connect to main network")
            else:
                get_logger().warning("‚ö†Ô∏è  No bootnodes returned from blockchain - check network connection")
                
        except Exception as e:
            get_logger().error(f"‚ùå Failed to get bootnodes from blockchain: {e}")
            get_logger().warning("‚ö†Ô∏è  This may indicate blockchain connection issues")

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º peer ID –≤ –±–ª–æ–∫—á–µ–π–Ω–µ
        get_logger().info("üîê Registering peer ID in blockchain...")
        try:
            self.coordinator.register_peer(self.peer_id)
            get_logger().info(f"‚úÖ Successfully registered peer ID [{self.peer_id}] in blockchain")
            get_logger().info("üîó Peer is now linked to blockchain identity")
        except Exception as e:
            get_logger().error(f"‚ùå Failed to register peer ID in blockchain: {e}")
            get_logger().warning("‚ö†Ô∏è  This may affect on-chain participation")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ä–∞—É–Ω–¥ –∏–∑ –±–ª–æ–∫—á–µ–π–Ω–∞
        try:
            get_logger().info("üìä Getting current round from blockchain...")
            round, stage = self.coordinator.get_round_and_stage()
            get_logger().info(f"‚úÖ Retrieved current round: {round}, stage: {stage}")
            
            self.state.round = round
            self.communication.step_ = self.state.round
            
            get_logger().info(f"üéØ Synchronized with blockchain - starting from round {round}")
            
        except Exception as e:
            get_logger().error(f"‚ùå Failed to get round/stage from blockchain: {e}")
            get_logger().warning("‚ö†Ô∏è  Using default round/stage values")

        get_logger().info(f"üê± Hello üêà [{get_name_from_peer_id(self.peer_id)}] ü¶Æ [{self.peer_id}]!")
        get_logger().info(f"üîó Bootnodes from config: {kwargs.get('bootnodes', [])}")

        # Safely get the model name first, then use it.
        model_name = "UnknownModel"
        
        # Check if we are in vLLM mode
        if hasattr(self.trainer, "use_vllm") and self.trainer.use_vllm:
            # In vLLM mode, use the name we saved in the trainer
            model_name = getattr(self.trainer, "model_name", "vLLM_Model")
            get_logger().info("üöÄ Running in vLLM mode - fast inference enabled")
        else:
            # In standard training mode, safely access the config attribute
            config_obj = getattr(getattr(self.trainer, "model", None), "config", None)
            if config_obj:
                model_name = getattr(config_obj, "_name_or_path", "UnknownModel")
            get_logger().info("üèãÔ∏è Running in standard training mode")
        
        get_logger().info(f"ü§ñ Using Model: {model_name}")

        # Enable push to HF if token was provided
        self.hf_token = hf_token
        if self.hf_token not in [None, "None"]:
            get_logger().info("ü§ó Setting up Hugging Face integration...")
            # This block should only run if we can actually push, which means we're in training mode.
            if not (hasattr(self.trainer, "use_vllm") and self.trainer.use_vllm):
                try:
                    username = whoami(token=self.hf_token)["name"]
                    model_name_suffix = model_name.split("/")[-1]
                    hub_model_id = f"{username}/{model_name_suffix}-Gensyn-Swarm-{self.animal_name}"
                    
                    self.trainer.args.hub_model_id = hub_model_id
                    self.trainer.args.push_to_hub = True
                    self.trainer.args.hub_token = self.hf_token
                    self.hf_push_frequency = hf_push_frequency
                    get_logger().info("Logging into Hugging Face Hub...")
                    login(self.hf_token)
                    get_logger().info(f"‚úÖ Hugging Face setup complete - model will push to {self.trainer.args.hub_model_id}")
                except Exception as e:
                    get_logger().warning(f"Could not set up Hugging Face push. Error: {e}")
            else:
                get_logger().info("Hugging Face push is disabled in vLLM mode.")
        else:
            get_logger().info("üì§ Hugging Face push disabled - no token provided")
        
        # –†–µ–∑—é–º–µ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        get_logger().info("üìã Connection Summary:")
        get_logger().info("  üîê Peer ID: Generated and registered ‚úÖ")
        get_logger().info("  üåê DHT Network: Initialized ‚úÖ")
        get_logger().info("  ‚õìÔ∏è  Blockchain: Connected ‚úÖ")
        get_logger().info("  üì° Bootnodes: Retrieved from blockchain ‚úÖ")
        get_logger().info("  üéØ Ready to participate in main swarm network!")

        with open(os.path.join(log_dir, f"system_info.txt"), "w") as f:
            f.write(get_system_info())

        self.batched_signals = 0.0
        self.time_since_submit = time.time() #seconds
        self.submit_period = 3.0 #hours
        self.submitted_this_round = False

    def _get_total_rewards_by_agent(self):
        rewards_by_agent = defaultdict(int)
        for stage in range(self.state.stage):
            rewards = self.rewards[stage]
            for agent_id, agent_rewards in rewards.items():
                for batch_id, batch_rewards in agent_rewards.items():
                    tot = 0
                    for generation_rewards in batch_rewards:
                        tot += sum(generation_rewards)
                    rewards_by_agent[agent_id] += tot

        return rewards_by_agent

    def _get_my_rewards(self, signal_by_agent):
        if len(signal_by_agent) == 0:
            return 0
        if self.peer_id in signal_by_agent:
            my_signal = signal_by_agent[self.peer_id]
        else:
            my_signal = 0
        my_signal = (my_signal + 1) * (my_signal > 0) + my_signal * (
            my_signal <= 0
        )
        return my_signal

    def _try_submit_to_chain(self, signal_by_agent):
        elapsed_time_hours = (time.time() - self.time_since_submit) / 3600
        if elapsed_time_hours > self.submit_period:
            try:
                self.coordinator.submit_reward(
                    self.state.round, 0, int(self.batched_signals), self.peer_id
                )
                self.batched_signals = 0.0
                if len(signal_by_agent) > 0:
                    max_agent, max_signal = max(signal_by_agent.items(), key=lambda x: x[1])
                else: # if we have no signal_by_agents, just submit ourselves.
                    max_agent = self.peer_id

                self.coordinator.submit_winners(self.state.round, [max_agent], self.peer_id)
                self.time_since_submit = time.time()
                self.submitted_this_round = True
            except Exception as e:
                get_logger().exception(
                    "Failed to submit to chain.\n"
                    "This is most likely transient and will recover.\n"
                    "There is no need to kill the program.\n"
                    "If you encounter this error, please report it to Gensyn by\n"
                    "filing a github issue here: https://github.com/gensyn-ai/rl-swarm/issues/ \n"
                    "including the full stacktrace."
                )

    def _hook_after_rewards_updated(self):
        signal_by_agent = self._get_total_rewards_by_agent()
        my_current_reward = self._get_my_rewards(signal_by_agent)
        self.batched_signals += my_current_reward
        get_logger().info(f"üéØ Current rewards for peer: {self.batched_signals:.2f} (added: {my_current_reward:.2f})")
        self._try_submit_to_chain(signal_by_agent)

    def _hook_after_round_advanced(self):
        self._save_to_hf()

        # Try to submit to chain again if necessary, but don't update our signal twice
        if not self.submitted_this_round:
            signal_by_agent = self._get_total_rewards_by_agent()
            self._try_submit_to_chain(signal_by_agent)
        
        # Reset flag for next round
        self.submitted_this_round = False

        # Block until swarm round advances
        self.agent_block()

    def _hook_after_game(self):
        self._save_to_hf()

    def _save_to_hf(self):
        if self.hf_token not in [None, "None"] and self.state.round % self.hf_push_frequency == 0:
            get_logger().info(f"pushing model to huggingface")
            try:
                repo_id = self.trainer.args.hub_model_id
                if repo_id is None:
                    repo_id = Path(self.trainer.args.output_dir).name

                self.trainer.model.push_to_hub(
                    repo_id=repo_id,
                    token=self.hf_token,
                    commit_message=f"rl-swarm: round {self.state.round}, agent {self.animal_name}",
                    tags=[
                        "rl-swarm",
                        "genrl-swarm",
                        "grpo",
                        "gensyn",
                        f"I am {self.animal_name}",
                    ],
                )
            except Exception:
                get_logger().exception(
                    "Failed to push model to the Hugging Face Hub. When you conclude training please try manually pushing it yourself using the instructions here: https://huggingface.co/docs/hub/en/models-uploading",
                    stack_info=True,
                )

    def agent_block(self, check_interval=5.0, log_timeout=10.0, max_check_interval=60.0 * 15):
        start_time = time.monotonic()
        fetch_log_time = start_time
        check_backoff = check_interval  # Exponential backoff for already finished rounds.
        while time.monotonic() - start_time < self.train_timeout:
            curr_time = time.monotonic()
            _ = self.communication.dht.get_visible_maddrs(latest=True)

            # Retrieve current round and stage.
            try:
                round_num, stage = self.coordinator.get_round_and_stage()
            except Exception as e:
                if curr_time - fetch_log_time > log_timeout:
                    get_logger().debug(
                        f"Could not fetch round and stage: {e}. Next check in {check_interval}s."
                    )
                    fetch_log_time = curr_time

                time.sleep(check_interval)
                continue

            if round_num >= self.state.round:
                get_logger().info(f"üêù Joining round: {round_num}")
                check_backoff = check_interval  # Reset backoff after successful round
                self.state.round = round_num  # advance to swarm's round.
                return
            else:
                get_logger().info(
                    f"Already finished round: {round_num}. Next check in {check_backoff}s."
                )
                time.sleep(check_backoff)
                check_backoff = min(check_backoff * 2, max_check_interval)

            if round_num == self.max_round - 1:
                return

        get_logger().info("Training timed out!")
